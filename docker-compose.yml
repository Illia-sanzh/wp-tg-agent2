# WordPress Telegram Agent - Container Stack
# Architecture: Bot → Agent → LiteLLM → Squid → Internet
#
# NETWORK ISOLATION MODEL (matches PDF architecture):
#   agent-internal: internal:true → NO route to internet. Containers here
#                   physically cannot open outbound connections to the internet.
#   proxy-external: normal bridge with internet routing. ONLY Squid is here.
#
#   Squid sits in BOTH networks, acting as the sole internet gateway.
#   HTTP_PROXY env vars ensure HTTP/HTTPS traffic uses Squid.
#   internal:true ensures raw-socket bypass attempts are blocked at the OS level.

services:

  # ── Egress proxy ──────────────────────────────────────────────────────────
  # THE ONLY CONTAINER WITH INTERNET ACCESS.
  # Lives in agent-internal (to serve other containers) AND proxy-external
  # (to reach the internet). Allowlist enforced by squid.conf.
  openclaw-squid:
    build:
      context: ./squid
      dockerfile: Dockerfile
    container_name: openclaw-squid
    restart: unless-stopped
    volumes:
      - ./squid/squid.conf:/etc/squid/squid.conf:ro
      - ./squid/allowlist.txt:/etc/squid/allowlist.txt:ro
      - squid-logs:/var/log/squid
    networks:
      agent-internal: {}   # Accessible to other containers
      proxy-external: {}   # Can reach the internet
    healthcheck:
      # bash /dev/tcp check: reliable port-open test with no extra tools needed.
      # squid -k check requires a PID file (not written in -N foreground mode).
      test: ["CMD-SHELL", "bash -c 'echo > /dev/tcp/localhost/3128' 2>/dev/null"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 20s

  # ── LLM Proxy ─────────────────────────────────────────────────────────────
  # Sits between agent and AI providers. Holds real API keys. Budget limits.
  # internal:true means it has NO internet route — must use Squid for all
  # outbound AI API calls.
  openclaw-litellm:
    image: ghcr.io/berriai/litellm:main-stable
    container_name: openclaw-litellm
    restart: unless-stopped
    volumes:
      - ./litellm/config.yaml:/app/config.yaml:ro
    env_file: .env
    command: ["--config", "/app/config.yaml", "--port", "4000", "--num_workers", "1"]
    environment:
      # Force all AI API calls through Squid (the only internet gateway)
      HTTPS_PROXY: "http://openclaw-squid:3128"
      HTTP_PROXY: "http://openclaw-squid:3128"
      NO_PROXY: "openclaw-squid,openclaw-agent,openclaw-bot,localhost,127.0.0.1,172.28.0.0/16"
      UVICORN_WORKERS: "1"
    mem_limit: 896m
    networks:
      agent-internal: {}   # No internet route (internal:true on this network)
    depends_on:
      openclaw-squid:
        condition: service_healthy
    healthcheck:
      # Python socket check — works regardless of which HTTP endpoints exist and
      # does not require curl/wget (absent from the Python slim base image).
      test: ["CMD-SHELL", "python3 -c 'import socket; s=socket.socket(); s.settimeout(3); s.connect((\"localhost\",4000)); s.close()'"]
      interval: 15s
      timeout: 10s
      retries: 12
      start_period: 60s

  # ── WordPress AI Agent ────────────────────────────────────────────────────
  # Receives tasks, runs agentic loop with the LLM, executes WP-CLI / REST API.
  # internal:true means it physically cannot reach the internet directly.
  # All outbound traffic goes through Squid via HTTP_PROXY env var.
  # LLM calls go to LiteLLM (internal), which then proxies through Squid.
  openclaw-agent:
    build:
      context: ./agent
      dockerfile: Dockerfile
    container_name: openclaw-agent
    restart: unless-stopped
    env_file: .env
    environment:
      # Points to LiteLLM, NOT directly to Anthropic → fixes the 401 issue
      LITELLM_BASE_URL: "http://openclaw-litellm:4000/v1"
      # For any tool that downloads from the internet (plugins, images)
      HTTPS_PROXY: "http://openclaw-squid:3128"
      HTTP_PROXY: "http://openclaw-squid:3128"
      NO_PROXY: "openclaw-litellm,openclaw-squid,openclaw-bot,host.docker.internal,localhost,127.0.0.1,172.28.0.0/16"
      PYTHONUNBUFFERED: "1"
      SKILL_FILE: "/app/SKILL.md"
      WP_PATH: "/wordpress"
      # Overrides DB_HOST inside the container so WP-CLI can reach the host's MariaDB.
      # wp-config.php reads this env var; PHP-FPM on the host doesn't have it set,
      # so WordPress itself keeps connecting via localhost (socket). See install.sh.
      WP_DB_HOST: "host.docker.internal"
    extra_hosts:
      # 172.28.0.1 is the gateway of the agent-internal bridge — the host machine's IP
      # as seen from containers on this network. Using host-gateway would resolve to
      # docker0 (172.17.0.1) which is unreachable from an internal:true network.
      - "host.docker.internal:172.28.0.1"
    volumes:
      # WordPress files (if on same server). Remote-only mode: set WP_PATH="" in .env
      - ${WP_PATH:-/var/www/html}:/wordpress:rw
      - ./SKILL.md:/app/SKILL.md:ro
      - ./openclaw-config:/app/config:ro
      - agent-tmp:/tmp/wp-agent
    mem_limit: 1152m
    networks:
      agent-internal: {}   # No internet route
    depends_on:
      openclaw-litellm:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:8080/health || exit 1"]
      interval: 20s
      timeout: 10s
      retries: 5
      start_period: 20s

  # ── OpenClaw Gateway ──────────────────────────────────────────────────────
  # Replaces the custom Telegram bot. Handles all messaging channels natively
  # (Telegram, WhatsApp, Discord, Slack, Signal, etc.) and exposes a web
  # dashboard at port 18789 (access via Tailscale or SSH tunnel only).
  #
  # The wordpress-manager skill (in ./openclaw-workspace/skills/) instructs
  # OpenClaw's agent to relay WordPress tasks to openclaw-agent:8080/ask.
  #
  # Config: ./openclaw-workspace/openclaw.json
  # Skills: ./openclaw-workspace/skills/
  #
  # Image: ghcr.io/openclaw/openclaw:latest
  # If no pre-built image is available, build from source:
  #   git clone https://github.com/openclaw/openclaw /tmp/openclaw-src
  #   docker build -t openclaw:local /tmp/openclaw-src
  #   Then change 'image:' below to 'openclaw:local'
  openclaw-gateway:
    image: ghcr.io/openclaw/openclaw:latest
    container_name: openclaw-gateway
    restart: unless-stopped
    volumes:
      # Workspace contains openclaw.json config + skills/
      - ./openclaw-workspace:/home/node/.openclaw:rw
    env_file: .env
    environment:
      # Route all outbound traffic (Telegram API, AI APIs) through Squid
      HTTPS_PROXY: "http://openclaw-squid:3128"
      HTTP_PROXY: "http://openclaw-squid:3128"
      NO_PROXY: "openclaw-agent,openclaw-litellm,openclaw-squid,localhost,127.0.0.1,172.28.0.0/16"
      # LiteLLM as the LLM backend (OpenAI-compatible)
      # If OpenClaw doesn't support custom base URLs, it falls back to ANTHROPIC_API_KEY
      OPENAI_API_KEY: "${LITELLM_MASTER_KEY}"
      OPENAI_BASE_URL: "http://openclaw-litellm:4000/v1"
    ports:
      # Web dashboard — bound to localhost only.
      # Access via: ssh -L 18789:localhost:18789 user@server
      # Or via Tailscale if configured.
      - "127.0.0.1:18789:18789"
    mem_limit: 512m
    networks:
      agent-internal: {}   # No direct internet; all traffic via Squid
    depends_on:
      openclaw-squid:
        condition: service_healthy
      openclaw-agent:
        condition: service_healthy

volumes:
  squid-logs:
  agent-tmp:

networks:
  # All agent/bot containers live here. internal:true = NO internet routing.
  # Containers can reach each other, but not the outside world directly.
  agent-internal:
    driver: bridge
    internal: true
    ipam:
      config:
        - subnet: "172.28.0.0/16"

  # Only Squid lives here alongside agent-internal.
  # Normal bridge = HAS internet routing.
  # This is Squid's "outside leg" for reaching AI APIs, Telegram, etc.
  proxy-external:
    driver: bridge
    ipam:
      config:
        - subnet: "172.29.0.0/16"
